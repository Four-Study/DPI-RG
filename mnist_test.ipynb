{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c393470",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load necessary modules\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.tools import *\n",
    "from utils.losses import *\n",
    "from models.mnist import *\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53617817",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "# torch.manual_seed(random_seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # check if gpu is available\n",
    "\n",
    "## load datasets\n",
    "# train_gen, dev_gen, test_gen = load(batch_size, batch_size)\n",
    "# data = inf_train_gen_mnist(train_gen)\n",
    "transform    = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_gen    = dsets.MNIST(root=\"./datasets\",train=True, transform=transform, download=True)\n",
    "test_gen     = dsets.MNIST(root=\"./datasets\",train=False, transform=transform, download=True)\n",
    "# train_loader = DataLoader(train_gen, batch_size=batch_size, shuffle=True)\n",
    "# test_loader  = DataLoader(test_gen, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "## initial empty lists for training progress\n",
    "# primal_loss_GI = []\n",
    "# dual_loss_GI = []\n",
    "# primal_loss = []\n",
    "# dual_loss = []\n",
    "# primal_loss_z = []\n",
    "# loss_mmd = []\n",
    "# gp = []\n",
    "# re = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1599a5-f845-4653-8753-d0c644d567ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper-parameters\n",
    "n_rep = 1\n",
    "random_seed = 2020\n",
    "epochs1 = 30\n",
    "epochs2 = 30\n",
    "std = 0.5\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0.01\n",
    "batch_size = 250\n",
    "nc = 1\n",
    "z_dim = 5\n",
    "# d_dim = 512\n",
    "# g_dim = 512\n",
    "# lambda_mmd = 10.0\n",
    "lambda_gp = 0.1\n",
    "lambda_power = 1.0\n",
    "present_label = list(range(10))\n",
    "missing_label = []\n",
    "all_label     = present_label + missing_label\n",
    "classes       = train_gen.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de72efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************\n",
    "# *** DPI-RG Algorithm ***\n",
    "# ************************\n",
    "\n",
    "cover_accs = []\n",
    "avg_counts = []\n",
    "\n",
    "# for rep in range(n_rep):\n",
    "T_trains = []\n",
    "for lab in present_label:\n",
    "    ## initialize models\n",
    "    netI = I_MNIST(num_classes=z_dim)\n",
    "    netG = G_MNIST(nz=z_dim)\n",
    "    netD = D_MNIST(nz=z_dim, power = 5)\n",
    "    netI = netI.to(device)\n",
    "    netG = netG.to(device)\n",
    "    netD = netD.to(device)\n",
    "    netI = nn.DataParallel(netI)\n",
    "    netG = nn.DataParallel(netG)\n",
    "    netD = nn.DataParallel(netD)\n",
    "    # model_save_file = 'mnist_param/' + 'class' + str(lab) + '.pt'\n",
    "    # netI.load_state_dict(torch.load(model_save_file))\n",
    "\n",
    "    ## set up optimizers\n",
    "    optim_I = optim.Adam(netI.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    optim_G = optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    optim_D = optim.Adam(netD.parameters(), lr=learning_rate * 5, betas=(0.5, 0.999), \n",
    "                         weight_decay=weight_decay)\n",
    "    ## filter data for each label and train them respectively\n",
    "    idxs = torch.where(train_gen.targets == lab)[0] \n",
    "    train_data = torch.utils.data.Subset(train_gen, idxs)\n",
    "    train_loader  = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    ## train for the first time\n",
    "    train_al(netI, netG, netD, optim_I, optim_G, optim_D,\n",
    "             train_gen, train_loader, batch_size, 0, epochs1, \n",
    "             z_dim, device, lab, present_label, all_label, \n",
    "             lambda_gp, lambda_power)\n",
    "\n",
    "    ## find out fake_zs\n",
    "    fake_zs = []\n",
    "    with torch.no_grad(): \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            x, _ = batch\n",
    "            fake_z = netI(x.to(device))\n",
    "            fake_zs.append(fake_z)\n",
    "    fake_zs = torch.cat(fake_zs)\n",
    "    ## get the empirical distribution for each label\n",
    "    T_train = torch.sqrt(torch.sum(fake_zs ** 2, dim=1) + 1)\n",
    "\n",
    "    ## get powers to determine new sample sizes\n",
    "    powers = []\n",
    "    for cur_lab in present_label:    \n",
    "        if cur_lab != lab:\n",
    "            # fake_Cs for this class\n",
    "            if torch.is_tensor(train_gen.targets):\n",
    "                idxs3 = torch.where(train_gen.targets == cur_lab)[0] \n",
    "            else:\n",
    "                idxs3 = torch.where(torch.Tensor(train_gen.targets) == cur_lab)[0] \n",
    "            train_data3 = torch.utils.data.Subset(train_gen, idxs3)\n",
    "            train_loader3  = DataLoader(train_data3, batch_size=batch_size, shuffle=False)\n",
    "            p_vals = torch.zeros(len(idxs3)) \n",
    "            fake_zs = torch.zeros(len(idxs3))\n",
    "            em_len = len(T_train)\n",
    "\n",
    "            for i, batch in enumerate(train_loader3):\n",
    "                x, _ = batch\n",
    "                fake_z = netI(x.to(device))\n",
    "                T_batch = torch.sqrt(torch.sum(fake_z ** 2, dim=1) + 1)\n",
    "\n",
    "                # compute p-value for each sample\n",
    "                for j in range(len(fake_z)):\n",
    "                    p1 = torch.sum(T_train > T_batch[j]) / em_len\n",
    "                    p = p1\n",
    "                    # calculate the p-value and put it in the corresponding list\n",
    "                    p_vals[i * batch_size + j] = p.item()\n",
    "            powers.append(np.sum(np.array(p_vals) <= 0.05) / len(idxs3))\n",
    "            \n",
    "    sample_sizes = max(powers) - powers + 0.05\n",
    "    sample_sizes = (sample_sizes / sum(sample_sizes) * len(idxs3)).astype(int)\n",
    "    ## train for the second time according to the calculated sample sizes\n",
    "    train_al(netI, netG, netD, optim_I, optim_G, optim_D,\n",
    "             train_gen, train_loader, batch_size, epochs1, epochs2, \n",
    "             z_dim, device, lab, present_label, all_label, \n",
    "             lambda_gp, lambda_power, sample_sizes)\n",
    "    \n",
    "    ## find out fake_zs\n",
    "    fake_zs = []\n",
    "    with torch.no_grad(): \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            x, _ = batch\n",
    "            fake_z = netI(x.to(device))\n",
    "            fake_zs.append(fake_z)\n",
    "    fake_zs = torch.cat(fake_zs)\n",
    "    ## get the empirical distribution for each label\n",
    "    T_train = torch.sqrt(torch.sum(fake_zs ** 2, dim=1) + 1)\n",
    "    T_trains.append(T_train)\n",
    "\n",
    "    ## save net and graphs for each label\n",
    "    model_save_file = 'mnist_param/' + 'class' + str(lab) + '.pt'\n",
    "    torch.save(netI.state_dict(), model_save_file)\n",
    "    del netI\n",
    "    print('Class', lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(primal_loss_GI)\n",
    "# print(dual_loss_GI)\n",
    "# print(loss_mmd)\n",
    "# print(gp)\n",
    "# print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bb3e3-7301-4263-a533-21e56b982358",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_p_vals  = []\n",
    "all_fake_zs = []\n",
    "\n",
    "for lab in all_label:    \n",
    "    if torch.is_tensor(test_gen.targets):\n",
    "        idxs2 = torch.where(test_gen.targets == lab)[0] \n",
    "    else:\n",
    "        idxs2 = torch.where(torch.Tensor(test_gen.targets) == lab)[0] \n",
    "    test_data = torch.utils.data.Subset(test_gen, idxs2)\n",
    "    test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # p_vals and fake_zs store p-values, fake_zs for the current iteration\n",
    "    fake_zs = torch.zeros(len(present_label), len(idxs2))\n",
    "    p_vals = torch.zeros(len(present_label), len(idxs2)) \n",
    "\n",
    "    for pidx in range(len(present_label)):\n",
    "        T_train = T_trains[pidx]\n",
    "        em_len = len(T_train)\n",
    "        netI = I_MNIST(num_classes=z_dim)\n",
    "        netI = netI.to(device)\n",
    "        netI = torch.nn.DataParallel(netI)\n",
    "        model_save_file = 'mnist_param/' + 'class' + str(present_label[pidx]) + '.pt'\n",
    "        netI.load_state_dict(torch.load(model_save_file))\n",
    "\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            images, y = batch\n",
    "            x = images.view(-1, 784).to(device)\n",
    "            fake_z = netI(x)\n",
    "            T_batch = torch.sqrt(torch.sum(torch.square(fake_z), 1) + 1) \n",
    "            ## compute p-value for each sample\n",
    "            for j in range(len(fake_z)):\n",
    "                p1 = torch.sum(T_train > T_batch[j]) / em_len\n",
    "                p = p1\n",
    "                # calculate the p-value and put it in the corresponding list\n",
    "                p_vals[pidx, i * batch_size + j] = p.item()\n",
    "\n",
    "    all_p_vals.append(np.array(p_vals))\n",
    "    ## concatenate torch data\n",
    "    all_fake_zs.append(np.array(fake_zs))\n",
    "    # print('Finished Label {}'.format(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b7d2a-f2b7-41c5-adbe-140aa48529af",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_p(all_p_vals, present_label, all_label, missing_label, z_dim, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
