{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c393470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02_16_2042\n"
     ]
    }
   ],
   "source": [
    "## load necessary modules\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.tools import *\n",
    "from utils.losses import *\n",
    "from models.cifar10 import *\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m_%d_%H%M\")\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53617817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # check if gpu is available\n",
    "\n",
    "## load datasets\n",
    "# train_gen, dev_gen, test_gen = load(batch_size, batch_size)\n",
    "# data = inf_train_gen_mnist(train_gen)\n",
    "transform    = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),  # Normalize the image\n",
    "                         (0.5, 0.5, 0.5))\n",
    "])\n",
    "train_gen    = dsets.CIFAR10(root=\"./datasets\",train=True, transform=transform, download=True)\n",
    "test_gen     = dsets.CIFAR10(root=\"./datasets\",train=False, transform=transform, download=True)\n",
    "# train_loader = DataLoader(train_gen, batch_size=batch_size, shuffle=True)\n",
    "# test_loader  = DataLoader(test_gen, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1599a5-f845-4653-8753-d0c644d567ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper-parameters\n",
    "n_rep = 1\n",
    "epochs1 = 50\n",
    "epochs2 = 50\n",
    "std = 0.1\n",
    "lr_GI = 2e-5\n",
    "lr_D = 1e-4\n",
    "weight_decay = 0.01\n",
    "batch_size = 250\n",
    "z_dim = 5\n",
    "lambda_mmd = 1.0\n",
    "lambda_gp = 0.1\n",
    "lambda_power = 0.6\n",
    "eta = 0.0\n",
    "present_label = list(range(10))\n",
    "missing_label = []\n",
    "all_label     = present_label + missing_label\n",
    "classes       = train_gen.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de72efba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GI: 2.414025\n",
      "MMD: 0.061850\n",
      "D: 0.001388\n",
      "gp: 0.081754\n",
      "power: 0.043468\n",
      "GI: 1.895977\n",
      "MMD: 0.041415\n",
      "D: -0.016375\n",
      "gp: 0.078484\n",
      "power: 0.033978\n",
      "GI: 1.533893\n",
      "MMD: 0.021671\n",
      "D: -0.000313\n",
      "gp: 0.078327\n",
      "power: 0.032312\n",
      "GI: 1.404056\n",
      "MMD: 0.021221\n",
      "D: -0.007261\n",
      "gp: 0.082081\n",
      "power: 0.027762\n",
      "GI: 1.283061\n",
      "MMD: 0.019388\n",
      "D: -0.001798\n",
      "gp: 0.079392\n",
      "power: 0.024166\n",
      "GI: 1.238168\n",
      "MMD: 0.020364\n",
      "D: -0.006988\n",
      "gp: 0.076218\n",
      "power: 0.023194\n",
      "GI: 1.091419\n",
      "MMD: 0.027706\n",
      "D: -0.006854\n",
      "gp: 0.079309\n",
      "power: 0.023258\n",
      "GI: 1.041238\n",
      "MMD: 0.026457\n",
      "D: -0.022245\n",
      "gp: 0.080165\n",
      "power: 0.022166\n",
      "GI: 1.000254\n",
      "MMD: 0.025999\n",
      "D: -0.011686\n",
      "gp: 0.076808\n",
      "power: 0.022294\n",
      "GI: 0.998105\n",
      "MMD: 0.023003\n",
      "D: -0.014069\n",
      "gp: 0.079786\n",
      "power: 0.022010\n",
      "GI: 0.977537\n",
      "MMD: 0.024668\n",
      "D: -0.022323\n",
      "gp: 0.076468\n",
      "power: 0.021798\n",
      "GI: 0.889499\n",
      "MMD: 0.028113\n",
      "D: -0.009361\n",
      "gp: 0.077654\n",
      "power: 0.021460\n",
      "GI: 0.820475\n",
      "MMD: 0.023508\n",
      "D: -0.016347\n",
      "gp: 0.080867\n",
      "power: 0.020716\n",
      "GI: 0.847514\n",
      "MMD: 0.022703\n",
      "D: -0.017392\n",
      "gp: 0.071891\n",
      "power: 0.020728\n",
      "GI: 0.796209\n",
      "MMD: 0.018622\n",
      "D: -0.014484\n",
      "gp: 0.077372\n",
      "power: 0.021463\n",
      "GI: 0.840316\n",
      "MMD: 0.017658\n",
      "D: -0.020153\n",
      "gp: 0.080729\n",
      "power: 0.023348\n",
      "GI: 0.752163\n",
      "MMD: 0.018531\n",
      "D: -0.002178\n",
      "gp: 0.076404\n",
      "power: 0.022786\n",
      "GI: 0.755038\n",
      "MMD: 0.017146\n",
      "D: -0.017581\n",
      "gp: 0.079302\n",
      "power: 0.023558\n",
      "GI: 0.695954\n",
      "MMD: 0.016571\n",
      "D: -0.025428\n",
      "gp: 0.074780\n",
      "power: 0.022323\n",
      "GI: 0.686209\n",
      "MMD: 0.010299\n",
      "D: -0.013969\n",
      "gp: 0.075043\n",
      "power: 0.023556\n",
      "GI: 0.660164\n",
      "MMD: 0.014415\n",
      "D: -0.005871\n",
      "gp: 0.075105\n",
      "power: 0.028325\n",
      "GI: 0.672103\n",
      "MMD: 0.011803\n",
      "D: -0.012530\n",
      "gp: 0.073050\n",
      "power: 0.024351\n",
      "GI: 0.648229\n",
      "MMD: 0.012477\n",
      "D: -0.014484\n",
      "gp: 0.074915\n",
      "power: 0.025331\n",
      "GI: 0.609711\n",
      "MMD: 0.010095\n",
      "D: -0.006862\n",
      "gp: 0.073563\n",
      "power: 0.025095\n",
      "GI: 0.586121\n",
      "MMD: 0.008489\n",
      "D: -0.017461\n",
      "gp: 0.071529\n",
      "power: 0.024347\n",
      "GI: 0.563624\n",
      "MMD: 0.008717\n",
      "D: -0.025207\n",
      "gp: 0.074801\n",
      "power: 0.024509\n",
      "GI: 0.528453\n",
      "MMD: 0.011409\n",
      "D: -0.006968\n",
      "gp: 0.076900\n",
      "power: 0.024988\n",
      "GI: 0.539354\n",
      "MMD: 0.008314\n",
      "D: -0.017066\n",
      "gp: 0.075404\n",
      "power: 0.026414\n",
      "GI: 0.570866\n",
      "MMD: 0.006564\n",
      "D: -0.007749\n",
      "gp: 0.070044\n",
      "power: 0.026396\n",
      "GI: 0.550862\n",
      "MMD: 0.004768\n",
      "D: -0.006379\n",
      "gp: 0.075311\n",
      "power: 0.025941\n",
      "GI: 0.624686\n",
      "MMD: 0.006881\n",
      "D: -0.024876\n",
      "gp: 0.076331\n",
      "power: 0.026942\n",
      "GI: 0.529758\n",
      "MMD: 0.004854\n",
      "D: -0.032296\n",
      "gp: 0.079509\n",
      "power: 0.027656\n",
      "GI: 0.507657\n",
      "MMD: 0.004270\n",
      "D: -0.017327\n",
      "gp: 0.073438\n",
      "power: 0.026884\n",
      "GI: 0.507230\n",
      "MMD: 0.005120\n",
      "D: -0.018245\n",
      "gp: 0.069006\n",
      "power: 0.027162\n",
      "GI: 0.504946\n",
      "MMD: 0.004381\n",
      "D: -0.027980\n",
      "gp: 0.068143\n",
      "power: 0.027460\n",
      "GI: 0.490938\n",
      "MMD: 0.003658\n",
      "D: -0.012276\n",
      "gp: 0.070023\n",
      "power: 0.026613\n",
      "GI: 0.485289\n",
      "MMD: 0.003537\n",
      "D: -0.002744\n",
      "gp: 0.070189\n",
      "power: 0.026411\n",
      "GI: 0.477267\n",
      "MMD: 0.004982\n",
      "D: 0.003284\n",
      "gp: 0.067868\n",
      "power: 0.026429\n",
      "GI: 0.478771\n",
      "MMD: 0.003010\n",
      "D: -0.008221\n",
      "gp: 0.075620\n",
      "power: 0.028846\n",
      "GI: 0.450886\n",
      "MMD: 0.002979\n",
      "D: 0.008185\n",
      "gp: 0.070110\n",
      "power: 0.029469\n",
      "GI: 0.453630\n",
      "MMD: 0.000959\n",
      "D: -0.018475\n",
      "gp: 0.069898\n",
      "power: 0.026852\n",
      "GI: 0.445877\n",
      "MMD: 0.002414\n",
      "D: -0.020428\n",
      "gp: 0.069655\n",
      "power: 0.030157\n",
      "GI: 0.423161\n",
      "MMD: 0.001306\n",
      "D: -0.006889\n",
      "gp: 0.066771\n",
      "power: 0.030888\n",
      "GI: 0.435661\n",
      "MMD: 0.002384\n",
      "D: 0.010295\n",
      "gp: 0.070413\n",
      "power: 0.028670\n",
      "GI: 0.437211\n",
      "MMD: 0.001169\n",
      "D: 0.004085\n",
      "gp: 0.073121\n",
      "power: 0.028323\n",
      "GI: 0.408759\n",
      "MMD: 0.002090\n",
      "D: 0.005701\n",
      "gp: 0.065367\n",
      "power: 0.028044\n",
      "GI: 0.413632\n",
      "MMD: 0.001720\n",
      "D: -0.006924\n",
      "gp: 0.064283\n",
      "power: 0.027923\n",
      "GI: 0.448293\n",
      "MMD: 0.001835\n",
      "D: 0.003919\n",
      "gp: 0.069329\n",
      "power: 0.029541\n",
      "GI: 0.450358\n",
      "MMD: 0.002389\n",
      "D: -0.013837\n",
      "gp: 0.074661\n",
      "power: 0.030460\n",
      "GI: 0.395201\n",
      "MMD: 0.002574\n",
      "D: 0.011089\n",
      "gp: 0.068263\n",
      "power: 0.028422\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Class 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GI: 2.274593\n",
      "MMD: 0.055780\n",
      "D: 0.001051\n",
      "gp: 0.079738\n",
      "power: 0.034859\n",
      "GI: 1.734876\n",
      "MMD: 0.020026\n",
      "D: 0.003623\n",
      "gp: 0.080284\n",
      "power: 0.027691\n",
      "GI: 1.516010\n",
      "MMD: 0.013966\n",
      "D: -0.001583\n",
      "gp: 0.081183\n",
      "power: 0.028322\n",
      "GI: 1.381928\n",
      "MMD: 0.014392\n",
      "D: 0.012899\n",
      "gp: 0.082438\n",
      "power: 0.025542\n",
      "GI: 1.230678\n",
      "MMD: 0.014475\n",
      "D: -0.003996\n",
      "gp: 0.082664\n",
      "power: 0.023367\n",
      "GI: 1.191355\n",
      "MMD: 0.024719\n",
      "D: -0.018113\n",
      "gp: 0.080526\n",
      "power: 0.023448\n",
      "GI: 1.145992\n",
      "MMD: 0.018106\n",
      "D: 0.003334\n",
      "gp: 0.082815\n",
      "power: 0.021924\n",
      "GI: 1.055758\n",
      "MMD: 0.021077\n",
      "D: -0.012738\n",
      "gp: 0.080693\n",
      "power: 0.021313\n",
      "GI: 1.014060\n",
      "MMD: 0.015075\n",
      "D: 0.001547\n",
      "gp: 0.079978\n",
      "power: 0.022727\n",
      "GI: 0.935161\n",
      "MMD: 0.017688\n",
      "D: -0.012770\n",
      "gp: 0.081618\n",
      "power: 0.022373\n",
      "GI: 0.954420\n",
      "MMD: 0.021543\n",
      "D: -0.021102\n",
      "gp: 0.075965\n",
      "power: 0.019977\n",
      "GI: 0.933395\n",
      "MMD: 0.025030\n",
      "D: -0.007750\n",
      "gp: 0.079953\n",
      "power: 0.021630\n",
      "GI: 0.836046\n",
      "MMD: 0.017967\n",
      "D: -0.020839\n",
      "gp: 0.080104\n",
      "power: 0.020906\n",
      "GI: 0.805348\n",
      "MMD: 0.017424\n",
      "D: -0.016469\n",
      "gp: 0.075777\n",
      "power: 0.020745\n",
      "GI: 0.788665\n",
      "MMD: 0.020916\n",
      "D: -0.020991\n",
      "gp: 0.072134\n",
      "power: 0.020628\n",
      "GI: 0.782863\n",
      "MMD: 0.017961\n",
      "D: -0.018154\n",
      "gp: 0.078352\n",
      "power: 0.022926\n",
      "GI: 0.799928\n",
      "MMD: 0.009653\n",
      "D: -0.012731\n",
      "gp: 0.076897\n",
      "power: 0.018840\n",
      "GI: 0.761619\n",
      "MMD: 0.011001\n",
      "D: -0.019540\n",
      "gp: 0.079491\n",
      "power: 0.020969\n",
      "GI: 0.673122\n",
      "MMD: 0.014488\n",
      "D: -0.013508\n",
      "gp: 0.073836\n",
      "power: 0.023351\n",
      "GI: 0.636964\n",
      "MMD: 0.014579\n",
      "D: -0.021489\n",
      "gp: 0.076640\n",
      "power: 0.022020\n",
      "GI: 0.641466\n",
      "MMD: 0.007092\n",
      "D: -0.012558\n",
      "gp: 0.073170\n",
      "power: 0.021043\n",
      "GI: 0.683089\n",
      "MMD: 0.009538\n",
      "D: -0.019108\n",
      "gp: 0.075652\n",
      "power: 0.021817\n",
      "GI: 0.660826\n",
      "MMD: 0.009921\n",
      "D: -0.015368\n",
      "gp: 0.076018\n",
      "power: 0.023425\n",
      "GI: 0.609206\n",
      "MMD: 0.014274\n",
      "D: -0.014508\n",
      "gp: 0.075066\n",
      "power: 0.024022\n",
      "GI: 0.614885\n",
      "MMD: 0.009699\n",
      "D: -0.022088\n",
      "gp: 0.077325\n",
      "power: 0.025249\n",
      "GI: 0.586659\n",
      "MMD: 0.009126\n",
      "D: -0.009734\n",
      "gp: 0.077792\n",
      "power: 0.023484\n",
      "GI: 0.550832\n",
      "MMD: 0.011339\n",
      "D: -0.015156\n",
      "gp: 0.075682\n",
      "power: 0.023955\n",
      "GI: 0.596406\n",
      "MMD: 0.016250\n",
      "D: 0.002967\n",
      "gp: 0.073236\n",
      "power: 0.025960\n",
      "GI: 0.572226\n",
      "MMD: 0.008540\n",
      "D: -0.007626\n",
      "gp: 0.073033\n",
      "power: 0.023595\n",
      "GI: 0.580659\n",
      "MMD: 0.008899\n",
      "D: -0.020644\n",
      "gp: 0.074118\n",
      "power: 0.023981\n",
      "GI: 0.481669\n",
      "MMD: 0.006843\n",
      "D: -0.002600\n",
      "gp: 0.078498\n",
      "power: 0.024571\n",
      "GI: 0.534168\n",
      "MMD: 0.006672\n",
      "D: -0.012536\n",
      "gp: 0.071323\n",
      "power: 0.023971\n",
      "GI: 0.522176\n",
      "MMD: 0.007013\n",
      "D: -0.003636\n",
      "gp: 0.075983\n",
      "power: 0.024659\n",
      "GI: 0.509749\n",
      "MMD: 0.005058\n",
      "D: -0.003425\n",
      "gp: 0.070674\n",
      "power: 0.025150\n",
      "GI: 0.478992\n",
      "MMD: 0.010990\n",
      "D: -0.007116\n",
      "gp: 0.070236\n",
      "power: 0.024760\n",
      "GI: 0.474993\n",
      "MMD: 0.007671\n",
      "D: -0.011126\n",
      "gp: 0.074366\n",
      "power: 0.024335\n",
      "GI: 0.494193\n",
      "MMD: 0.009722\n",
      "D: 0.000892\n",
      "gp: 0.072062\n",
      "power: 0.024437\n",
      "GI: 0.512468\n",
      "MMD: 0.007854\n",
      "D: -0.028435\n",
      "gp: 0.069384\n",
      "power: 0.022644\n",
      "GI: 0.480005\n",
      "MMD: 0.005739\n",
      "D: -0.015618\n",
      "gp: 0.073043\n",
      "power: 0.027171\n",
      "GI: 0.471848\n",
      "MMD: 0.003721\n",
      "D: -0.013137\n",
      "gp: 0.074488\n",
      "power: 0.028740\n",
      "GI: 0.480013\n",
      "MMD: 0.008034\n",
      "D: -0.017765\n",
      "gp: 0.072964\n",
      "power: 0.026666\n",
      "GI: 0.468038\n",
      "MMD: 0.005436\n",
      "D: -0.026531\n",
      "gp: 0.075522\n",
      "power: 0.024953\n",
      "GI: 0.490360\n",
      "MMD: 0.006476\n",
      "D: 0.006006\n",
      "gp: 0.073061\n",
      "power: 0.025901\n",
      "GI: 0.448561\n",
      "MMD: 0.008159\n",
      "D: -0.016699\n",
      "gp: 0.071390\n",
      "power: 0.028105\n",
      "GI: 0.524221\n",
      "MMD: 0.004843\n",
      "D: -0.008402\n",
      "gp: 0.072838\n",
      "power: 0.025210\n",
      "GI: 0.427855\n",
      "MMD: 0.004359\n",
      "D: -0.010448\n",
      "gp: 0.069112\n",
      "power: 0.027076\n",
      "GI: 0.434638\n",
      "MMD: 0.002998\n",
      "D: -0.032046\n",
      "gp: 0.067933\n",
      "power: 0.024923\n",
      "GI: 0.464819\n",
      "MMD: 0.003619\n",
      "D: -0.013246\n",
      "gp: 0.071079\n",
      "power: 0.028481\n",
      "GI: 0.488327\n",
      "MMD: 0.005216\n",
      "D: -0.008756\n",
      "gp: 0.070160\n",
      "power: 0.027076\n",
      "GI: 0.427679\n",
      "MMD: 0.001223\n",
      "D: -0.012046\n",
      "gp: 0.070926\n",
      "power: 0.032632\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Class 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GI: 2.272135\n",
      "MMD: 0.061240\n",
      "D: -0.010304\n",
      "gp: 0.085879\n",
      "power: 0.034733\n",
      "GI: 1.903300\n",
      "MMD: 0.035470\n",
      "D: 0.001045\n",
      "gp: 0.082005\n",
      "power: 0.030884\n",
      "GI: 1.547946\n",
      "MMD: 0.018894\n",
      "D: -0.009836\n",
      "gp: 0.079848\n",
      "power: 0.026088\n",
      "GI: 1.417946\n",
      "MMD: 0.023063\n",
      "D: -0.002261\n",
      "gp: 0.081512\n",
      "power: 0.026927\n",
      "GI: 1.342880\n",
      "MMD: 0.018932\n",
      "D: 0.001046\n",
      "gp: 0.081495\n",
      "power: 0.025167\n",
      "GI: 1.211360\n",
      "MMD: 0.022306\n",
      "D: -0.010818\n",
      "gp: 0.079756\n",
      "power: 0.023418\n",
      "GI: 1.157551\n",
      "MMD: 0.030529\n",
      "D: -0.020131\n",
      "gp: 0.080838\n",
      "power: 0.021881\n",
      "GI: 1.109739\n",
      "MMD: 0.026554\n",
      "D: -0.018794\n",
      "gp: 0.078417\n",
      "power: 0.021448\n",
      "GI: 0.999606\n",
      "MMD: 0.029395\n",
      "D: -0.018705\n",
      "gp: 0.081345\n",
      "power: 0.019266\n",
      "GI: 0.981005\n",
      "MMD: 0.031778\n",
      "D: -0.023727\n",
      "gp: 0.080825\n",
      "power: 0.018555\n",
      "GI: 0.977252\n",
      "MMD: 0.027779\n",
      "D: -0.040906\n",
      "gp: 0.081412\n",
      "power: 0.021237\n",
      "GI: 0.921364\n",
      "MMD: 0.026262\n",
      "D: -0.010794\n",
      "gp: 0.081013\n",
      "power: 0.021124\n",
      "GI: 0.850578\n",
      "MMD: 0.020399\n",
      "D: -0.005555\n",
      "gp: 0.077335\n",
      "power: 0.021030\n",
      "GI: 0.829521\n",
      "MMD: 0.024912\n",
      "D: -0.023383\n",
      "gp: 0.080713\n",
      "power: 0.022721\n",
      "GI: 0.819431\n",
      "MMD: 0.020138\n",
      "D: 0.004355\n",
      "gp: 0.080217\n",
      "power: 0.020609\n",
      "GI: 0.817157\n",
      "MMD: 0.023720\n",
      "D: -0.023052\n",
      "gp: 0.078710\n",
      "power: 0.021771\n",
      "GI: 0.795451\n",
      "MMD: 0.022217\n",
      "D: -0.003725\n",
      "gp: 0.075642\n",
      "power: 0.019948\n",
      "GI: 0.752289\n",
      "MMD: 0.018680\n",
      "D: -0.025533\n",
      "gp: 0.081106\n",
      "power: 0.021099\n",
      "GI: 0.737599\n",
      "MMD: 0.016272\n",
      "D: -0.007717\n",
      "gp: 0.077505\n",
      "power: 0.023111\n",
      "GI: 0.658999\n",
      "MMD: 0.019996\n",
      "D: -0.011857\n",
      "gp: 0.073421\n",
      "power: 0.022370\n",
      "GI: 0.683958\n",
      "MMD: 0.020671\n",
      "D: -0.010482\n",
      "gp: 0.075538\n",
      "power: 0.021369\n",
      "GI: 0.686217\n",
      "MMD: 0.014349\n",
      "D: -0.009441\n",
      "gp: 0.072349\n",
      "power: 0.023583\n",
      "GI: 0.626552\n",
      "MMD: 0.012868\n",
      "D: -0.005361\n",
      "gp: 0.074468\n",
      "power: 0.021395\n",
      "GI: 0.633371\n",
      "MMD: 0.011847\n",
      "D: -0.021021\n",
      "gp: 0.074738\n",
      "power: 0.023277\n",
      "GI: 0.573970\n",
      "MMD: 0.012427\n",
      "D: -0.019706\n",
      "gp: 0.081450\n",
      "power: 0.023041\n",
      "GI: 0.603394\n",
      "MMD: 0.011876\n",
      "D: -0.011876\n",
      "gp: 0.074567\n",
      "power: 0.024974\n",
      "GI: 0.586005\n",
      "MMD: 0.008830\n",
      "D: -0.001157\n",
      "gp: 0.075283\n",
      "power: 0.023961\n",
      "GI: 0.565190\n",
      "MMD: 0.008027\n",
      "D: -0.023931\n",
      "gp: 0.073631\n",
      "power: 0.023605\n",
      "GI: 0.599743\n",
      "MMD: 0.007954\n",
      "D: -0.015868\n",
      "gp: 0.073274\n",
      "power: 0.023575\n",
      "GI: 0.566859\n",
      "MMD: 0.007104\n",
      "D: -0.009166\n",
      "gp: 0.074837\n",
      "power: 0.025238\n",
      "GI: 0.540190\n",
      "MMD: 0.009537\n",
      "D: -0.017478\n",
      "gp: 0.069882\n",
      "power: 0.025470\n",
      "GI: 0.617719\n",
      "MMD: 0.007628\n",
      "D: -0.025126\n",
      "gp: 0.073056\n",
      "power: 0.024383\n",
      "GI: 0.510938\n",
      "MMD: 0.008353\n",
      "D: -0.013000\n",
      "gp: 0.070817\n",
      "power: 0.023726\n",
      "GI: 0.485246\n",
      "MMD: 0.007711\n",
      "D: -0.014372\n",
      "gp: 0.073287\n",
      "power: 0.026008\n",
      "GI: 0.514862\n",
      "MMD: 0.006363\n",
      "D: -0.012372\n",
      "gp: 0.071329\n",
      "power: 0.025888\n",
      "GI: 0.550486\n",
      "MMD: 0.005637\n",
      "D: -0.006416\n",
      "gp: 0.069158\n",
      "power: 0.025797\n",
      "GI: 0.533508\n",
      "MMD: 0.006575\n",
      "D: -0.009926\n",
      "gp: 0.068174\n",
      "power: 0.028979\n",
      "GI: 0.485298\n",
      "MMD: 0.005377\n",
      "D: -0.006696\n",
      "gp: 0.074864\n",
      "power: 0.026691\n",
      "GI: 0.494234\n",
      "MMD: 0.002666\n",
      "D: -0.011550\n",
      "gp: 0.066899\n",
      "power: 0.026070\n",
      "GI: 0.438096\n",
      "MMD: 0.003131\n",
      "D: -0.026645\n",
      "gp: 0.071353\n",
      "power: 0.028591\n",
      "GI: 0.458317\n",
      "MMD: 0.005817\n",
      "D: -0.014731\n",
      "gp: 0.071757\n",
      "power: 0.027114\n",
      "GI: 0.472205\n",
      "MMD: 0.003263\n",
      "D: -0.014740\n",
      "gp: 0.071406\n",
      "power: 0.027680\n",
      "GI: 0.487015\n",
      "MMD: 0.001814\n",
      "D: 0.002159\n",
      "gp: 0.072060\n",
      "power: 0.029262\n",
      "GI: 0.419917\n",
      "MMD: 0.004527\n",
      "D: -0.003274\n",
      "gp: 0.066342\n",
      "power: 0.029229\n",
      "GI: 0.432549\n",
      "MMD: 0.000817\n",
      "D: -0.018982\n",
      "gp: 0.071071\n",
      "power: 0.026293\n",
      "GI: 0.441104\n",
      "MMD: 0.000630\n",
      "D: 0.000782\n",
      "gp: 0.069452\n",
      "power: 0.028025\n",
      "GI: 0.521514\n",
      "MMD: 0.001620\n",
      "D: -0.008353\n",
      "gp: 0.073341\n",
      "power: 0.029541\n",
      "GI: 0.428977\n",
      "MMD: 0.002235\n",
      "D: -0.005992\n",
      "gp: 0.064114\n",
      "power: 0.029235\n",
      "GI: 0.447036\n",
      "MMD: 0.000636\n",
      "D: -0.004764\n",
      "gp: 0.067657\n",
      "power: 0.029154\n",
      "GI: 0.433588\n",
      "MMD: 0.002867\n",
      "D: -0.013594\n",
      "gp: 0.073707\n",
      "power: 0.028708\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Class 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GI: 2.171340\n",
      "MMD: 0.049353\n",
      "D: 0.005329\n",
      "gp: 0.080361\n",
      "power: 0.028348\n",
      "GI: 1.742214\n",
      "MMD: 0.019517\n",
      "D: -0.007059\n",
      "gp: 0.080574\n",
      "power: 0.027033\n",
      "GI: 1.491990\n",
      "MMD: 0.013837\n",
      "D: 0.022794\n",
      "gp: 0.082252\n",
      "power: 0.026081\n",
      "GI: 1.360848\n",
      "MMD: 0.020621\n",
      "D: -0.000810\n",
      "gp: 0.082624\n",
      "power: 0.025827\n",
      "GI: 1.204816\n",
      "MMD: 0.017757\n",
      "D: -0.000362\n",
      "gp: 0.080379\n",
      "power: 0.025130\n",
      "GI: 1.144672\n",
      "MMD: 0.029914\n",
      "D: 0.008310\n",
      "gp: 0.077729\n",
      "power: 0.024222\n",
      "GI: 1.095354\n",
      "MMD: 0.027322\n",
      "D: -0.000079\n",
      "gp: 0.078934\n",
      "power: 0.023610\n",
      "GI: 1.079988\n",
      "MMD: 0.027039\n",
      "D: -0.001715\n",
      "gp: 0.082267\n",
      "power: 0.021689\n",
      "GI: 1.040980\n",
      "MMD: 0.034840\n",
      "D: -0.008353\n",
      "gp: 0.082361\n",
      "power: 0.021383\n",
      "GI: 0.998262\n",
      "MMD: 0.024300\n",
      "D: -0.007912\n",
      "gp: 0.079639\n",
      "power: 0.020599\n",
      "GI: 0.924144\n",
      "MMD: 0.033086\n",
      "D: -0.011715\n",
      "gp: 0.081371\n",
      "power: 0.021480\n",
      "GI: 0.907099\n",
      "MMD: 0.026917\n",
      "D: -0.002283\n",
      "gp: 0.078646\n",
      "power: 0.020697\n",
      "GI: 0.878014\n",
      "MMD: 0.033319\n",
      "D: -0.003094\n",
      "gp: 0.081591\n",
      "power: 0.022082\n",
      "GI: 0.790031\n",
      "MMD: 0.024261\n",
      "D: -0.024080\n",
      "gp: 0.080458\n",
      "power: 0.019222\n",
      "GI: 0.798552\n",
      "MMD: 0.024902\n",
      "D: -0.022908\n",
      "gp: 0.078439\n",
      "power: 0.019334\n",
      "GI: 0.800441\n",
      "MMD: 0.021208\n",
      "D: -0.015207\n",
      "gp: 0.078568\n",
      "power: 0.022306\n",
      "GI: 0.750302\n",
      "MMD: 0.027761\n",
      "D: -0.015785\n",
      "gp: 0.081609\n",
      "power: 0.023489\n",
      "GI: 0.724014\n",
      "MMD: 0.018944\n",
      "D: -0.005850\n",
      "gp: 0.078016\n",
      "power: 0.023569\n",
      "GI: 0.687823\n",
      "MMD: 0.021743\n",
      "D: -0.004636\n",
      "gp: 0.080866\n",
      "power: 0.019681\n",
      "GI: 0.712538\n",
      "MMD: 0.022571\n",
      "D: -0.018700\n",
      "gp: 0.081245\n",
      "power: 0.021152\n",
      "GI: 0.644695\n",
      "MMD: 0.016694\n",
      "D: -0.011613\n",
      "gp: 0.078135\n",
      "power: 0.024147\n",
      "GI: 0.669750\n",
      "MMD: 0.013277\n",
      "D: -0.023616\n",
      "gp: 0.082647\n",
      "power: 0.023281\n",
      "GI: 0.638119\n",
      "MMD: 0.013855\n",
      "D: -0.021549\n",
      "gp: 0.081527\n",
      "power: 0.023738\n",
      "GI: 0.644178\n",
      "MMD: 0.015283\n",
      "D: -0.018089\n",
      "gp: 0.079589\n",
      "power: 0.023242\n",
      "GI: 0.638627\n",
      "MMD: 0.010649\n",
      "D: -0.009734\n",
      "gp: 0.078571\n",
      "power: 0.022097\n",
      "GI: 0.603943\n",
      "MMD: 0.007709\n",
      "D: -0.018781\n",
      "gp: 0.077054\n",
      "power: 0.023883\n",
      "GI: 0.566794\n",
      "MMD: 0.012681\n",
      "D: -0.010572\n",
      "gp: 0.080630\n",
      "power: 0.022645\n",
      "GI: 0.580441\n",
      "MMD: 0.014651\n",
      "D: -0.018834\n",
      "gp: 0.075273\n",
      "power: 0.026794\n",
      "GI: 0.572446\n",
      "MMD: 0.013496\n",
      "D: -0.030626\n",
      "gp: 0.080364\n",
      "power: 0.024461\n",
      "GI: 0.556796\n",
      "MMD: 0.013561\n",
      "D: -0.026932\n",
      "gp: 0.074277\n",
      "power: 0.024429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m train_loader  \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m## train for the first time\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m train_al(netI, netG, netD, optim_I, optim_G, optim_D,\n\u001b[1;32m     37\u001b[0m          train_gen, train_loader, batch_size, \u001b[38;5;241m0\u001b[39m, epochs1, \n\u001b[1;32m     38\u001b[0m          z_dim, device, lab, present_label, all_label, \n\u001b[1;32m     39\u001b[0m          lambda_gp, lambda_power, lambda_mmd \u001b[38;5;241m=\u001b[39m lambda_mmd,\n\u001b[1;32m     40\u001b[0m          img_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, nc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, eta \u001b[38;5;241m=\u001b[39m eta, \n\u001b[1;32m     41\u001b[0m          lr_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, trace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m## find out fake_zs\u001b[39;00m\n\u001b[1;32m     44\u001b[0m fake_zs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Git/FPI/utils/tools.py:77\u001b[0m, in \u001b[0;36mtrain_al\u001b[0;34m(netI, netG, netD, optim_I, optim_G, optim_D, train_gen, train_loader, batch_size, start_epoch, end_epoch, z_dim, device, lab, present_label, all_label, lambda_gp, lambda_power, lambda_mmd, eta, sample_sizes, sampled_idxs, img_size, nc, critic_iter, critic_iter_d, lr_decay, trace)\u001b[0m\n\u001b[1;32m     75\u001b[0m mmd \u001b[38;5;241m=\u001b[39m mmd_penalty(fake_z, z, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRBF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m primal_cost \u001b[38;5;241m=\u001b[39m cost_GI \u001b[38;5;241m+\u001b[39m lambda_mmd \u001b[38;5;241m*\u001b[39m mmd\n\u001b[0;32m---> 77\u001b[0m primal_cost\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     78\u001b[0m optim_I\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     79\u001b[0m optim_G\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ************************\n",
    "# *** DPI-RG Algorithm ***\n",
    "# ************************\n",
    "\n",
    "cover_accs = []\n",
    "avg_counts = []\n",
    "\n",
    "# for rep in range(n_rep):\n",
    "T_trains = []\n",
    "for lab in present_label:\n",
    "    ## initialize models\n",
    "    netI = I_CIFAR10_2(nz=z_dim)\n",
    "    netG = G_CIFAR10(nz=z_dim, ngf=64)\n",
    "    netD = D_CIFAR10(nz=z_dim, ndf=64)\n",
    "    netI = netI.to(device)\n",
    "    netG = netG.to(device)\n",
    "    netD = netD.to(device)\n",
    "    netI = nn.DataParallel(netI)\n",
    "    netG = nn.DataParallel(netG)\n",
    "    netD = nn.DataParallel(netD)\n",
    "\n",
    "    ## set up optimizers\n",
    "    optim_I = optim.Adam(netI.parameters(), lr=lr_GI, betas=(0.5, 0.999))\n",
    "    optim_G = optim.Adam(netG.parameters(), lr=lr_GI, betas=(0.5, 0.999))\n",
    "    optim_D = optim.Adam(netD.parameters(), lr=lr_D, betas=(0.5, 0.999), \n",
    "                         weight_decay=weight_decay)\n",
    "    ## filter data for each label and train them respectively\n",
    "    if torch.is_tensor(train_gen.targets):\n",
    "        idxs = torch.where(train_gen.targets == lab)[0] \n",
    "    else:\n",
    "        idxs = torch.where(torch.Tensor(train_gen.targets) == lab)[0] \n",
    "    train_data = torch.utils.data.Subset(train_gen, idxs)\n",
    "    train_loader  = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    ## train for the first time\n",
    "    train_al(netI, netG, netD, optim_I, optim_G, optim_D,\n",
    "             train_gen, train_loader, batch_size, 0, epochs1, \n",
    "             z_dim, device, lab, present_label, all_label, \n",
    "             lambda_gp, lambda_power, lambda_mmd = lambda_mmd,\n",
    "             img_size = 32, nc = 3, eta = eta, \n",
    "             lr_decay = None, trace=True)\n",
    "\n",
    "    ## find out fake_zs\n",
    "    fake_zs = []\n",
    "    with torch.no_grad(): \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            x, _ = batch\n",
    "            fake_z = netI(x.to(device))\n",
    "            fake_zs.append(fake_z)\n",
    "    fake_zs = torch.cat(fake_zs)\n",
    "    ## get the empirical distribution for each label\n",
    "    T_train = torch.sqrt(torch.sum(fake_zs ** 2, dim=1) + 1)\n",
    "\n",
    "    ## get powers to determine new sample sizes\n",
    "    powers = []\n",
    "    for cur_lab in present_label:    \n",
    "        if cur_lab != lab:\n",
    "            # fake_Cs for this class\n",
    "            if torch.is_tensor(train_gen.targets):\n",
    "                idxs3 = torch.where(train_gen.targets == cur_lab)[0] \n",
    "            else:\n",
    "                idxs3 = torch.where(torch.Tensor(train_gen.targets) == cur_lab)[0] \n",
    "            train_data3 = torch.utils.data.Subset(train_gen, idxs3)\n",
    "            train_loader3  = DataLoader(train_data3, batch_size=batch_size, shuffle=False)\n",
    "            p_vals = torch.zeros(len(idxs3)) \n",
    "            fake_zs = torch.zeros(len(idxs3))\n",
    "            em_len = len(T_train)\n",
    "\n",
    "            for i, batch in enumerate(train_loader3):\n",
    "                x, _ = batch\n",
    "                fake_z = netI(x.to(device))\n",
    "                T_batch = torch.sqrt(torch.sum(fake_z ** 2, dim=1) + 1)\n",
    "\n",
    "                # compute p-value for each sample\n",
    "                for j in range(len(fake_z)):\n",
    "                    p1 = torch.sum(T_train > T_batch[j]) / em_len\n",
    "                    p = p1\n",
    "                    # calculate the p-value and put it in the corresponding list\n",
    "                    p_vals[i * batch_size + j] = p.item()\n",
    "            powers.append(np.sum(np.array(p_vals) <= 0.05) / len(idxs3))\n",
    "            \n",
    "    sample_sizes = max(powers) - powers + 0.05\n",
    "    sample_sizes = (sample_sizes / sum(sample_sizes) * len(idxs3)).astype(int)\n",
    "    # print(sample_sizes)\n",
    "    ## train for the second time according to the calculated sample sizes\n",
    "    train_al(netI, netG, netD, optim_I, optim_G, optim_D,\n",
    "             train_gen, train_loader, batch_size, epochs1, epochs2, \n",
    "             z_dim, device, lab, present_label, all_label, \n",
    "             lambda_gp, lambda_power, lambda_mmd = lambda_mmd, sample_sizes = sample_sizes, \n",
    "             img_size = 32, nc = 3, eta = eta, \n",
    "             lr_decay = 10, trace = True)\n",
    "    \n",
    "    ## find out fake_zs\n",
    "    fake_zs = []\n",
    "    with torch.no_grad(): \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            x, _ = batch\n",
    "            fake_z = netI(x.to(device))\n",
    "            fake_zs.append(fake_z)\n",
    "    fake_zs = torch.cat(fake_zs)\n",
    "    ## get the empirical distribution for each label\n",
    "    T_train = torch.sqrt(torch.sum(fake_zs ** 2, dim=1) + 1)\n",
    "    T_trains.append(T_train)\n",
    "\n",
    "    ## save net and graphs for each label\n",
    "    model_save_file = f'cifar10_param/{timestamp}_class{lab}.pt'\n",
    "    torch.save(netI.state_dict(), model_save_file)\n",
    "    del netI\n",
    "    print('-'*100)\n",
    "    print('Class', lab)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef165e1-7090-454d-ac94-d74eec332227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for verification\n",
    "all_p_vals  = []\n",
    "all_fake_Ts = []\n",
    "\n",
    "for lab in all_label:    \n",
    "    if torch.is_tensor(train_gen.targets):\n",
    "        idxs2 = torch.where(train_gen.targets == lab)[0] \n",
    "    else:\n",
    "        idxs2 = torch.where(torch.Tensor(train_gen.targets) == lab)[0] \n",
    "    test_data = torch.utils.data.Subset(train_gen, idxs2)\n",
    "    test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # p_vals and fake_zs store p-values, fake_zs for the current iteration\n",
    "    fake_Ts = torch.zeros(len(present_label), len(idxs2))\n",
    "    p_vals = torch.zeros(len(present_label), len(idxs2)) \n",
    "\n",
    "    for pidx in range(len(present_label)):\n",
    "        T_train = T_trains[pidx]\n",
    "        em_len = len(T_train)\n",
    "        netI = I_CIFAR10_2(nz=z_dim)\n",
    "        netI = netI.to(device)\n",
    "        netI = torch.nn.DataParallel(netI)\n",
    "        model_save_file = f'cifar10_param/{timestamp}_class{present_label[pidx]}.pt'\n",
    "        netI.load_state_dict(torch.load(model_save_file))\n",
    "        \n",
    "        for i, batch in enumerate(test_loader):\n",
    "            images, y = batch\n",
    "            x = images.view(-1, 3, 32 * 32).to(device)\n",
    "            fake_z = netI(x)\n",
    "            T_batch = torch.sqrt(torch.sum(torch.square(fake_z), 1) + 1) \n",
    "            ## compute p-value for each sample\n",
    "            for j in range(len(fake_z)):\n",
    "                p1 = torch.sum(T_train > T_batch[j]) / em_len\n",
    "                p2 = torch.sum(T_train < T_batch[j]) / em_len\n",
    "                p = 2*min(p1, p2)\n",
    "                # calculate the p-value and put it in the corresponding list\n",
    "                fake_Ts[pidx, i * batch_size + j] = T_batch[j].item()\n",
    "                p_vals[pidx, i * batch_size + j] = p.item()\n",
    "\n",
    "    all_p_vals.append(np.array(p_vals))\n",
    "    ## concatenate torch data\n",
    "    all_fake_Ts.append(np.array(fake_Ts))\n",
    "    # print('Finished Label {}'.format(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f6b7f-a433-41e2-9de5-f313748dd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_T(all_fake_Ts, present_label, all_label, missing_label, z_dim, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5650706-a3e7-47f2-ace7-4d4dd0c340dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_p(all_p_vals, present_label, all_label, missing_label, z_dim, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bb3e3-7301-4263-a533-21e56b982358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test data set\n",
    "all_p_vals  = []\n",
    "all_fake_Ts = []\n",
    "\n",
    "for lab in all_label:    \n",
    "    if torch.is_tensor(test_gen.targets):\n",
    "        idxs2 = torch.where(test_gen.targets == lab)[0] \n",
    "    else:\n",
    "        idxs2 = torch.where(torch.Tensor(test_gen.targets) == lab)[0] \n",
    "    test_data = torch.utils.data.Subset(test_gen, idxs2)\n",
    "    test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # p_vals and fake_zs store p-values, fake_zs for the current iteration\n",
    "    fake_Ts = torch.zeros(len(present_label), len(idxs2))\n",
    "    p_vals = torch.zeros(len(present_label), len(idxs2)) \n",
    "\n",
    "    for pidx in range(len(present_label)):\n",
    "        T_train = T_trains[pidx]\n",
    "        em_len = len(T_train)\n",
    "        netI = I_CIFAR10_2(nz=z_dim)\n",
    "        netI = netI.to(device)\n",
    "        netI = torch.nn.DataParallel(netI)\n",
    "        model_save_file = f'cifar10_param/{timestamp}_class{present_label[pidx]}.pt'\n",
    "        netI.load_state_dict(torch.load(model_save_file))\n",
    "        \n",
    "        for i, batch in enumerate(test_loader):\n",
    "            images, y = batch\n",
    "            x = images.view(-1, 3, 32 * 32).to(device)\n",
    "            fake_z = netI(x)\n",
    "            T_batch = torch.sqrt(torch.sum(torch.square(fake_z), 1) + 1) \n",
    "            ## compute p-value for each sample\n",
    "            for j in range(len(fake_z)):\n",
    "                p1 = torch.sum(T_train > T_batch[j]) / em_len\n",
    "                p2 = torch.sum(T_train < T_batch[j]) / em_len\n",
    "                p = 2*min(p1, p2)\n",
    "                # calculate the p-value and put it in the corresponding list\n",
    "                fake_Ts[pidx, i * batch_size + j] = T_batch[j].item()\n",
    "                p_vals[pidx, i * batch_size + j] = p.item()\n",
    "\n",
    "    all_p_vals.append(np.array(p_vals))\n",
    "    ## concatenate torch data\n",
    "    all_fake_Ts.append(np.array(fake_Ts))\n",
    "    # print('Finished Label {}'.format(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a96a0-c2e2-4ccc-8c8a-8d9314de1c08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_T(all_fake_Ts, present_label, all_label, missing_label, z_dim, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b7d2a-f2b7-41c5-adbe-140aa48529af",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_p(all_p_vals, present_label, all_label, missing_label, z_dim, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c52517-29f4-472f-9064-2ae64561f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_accs = []\n",
    "avg_counts = []\n",
    "\n",
    "cover_acc = torch.zeros(len(all_label))\n",
    "avg_count = torch.zeros(len(all_label))\n",
    "for i, lab in enumerate(all_label):\n",
    "    p_vals = all_p_vals[i]\n",
    "    n = p_vals.shape[1]\n",
    "    cover = 0.0\n",
    "    counts = 0.0\n",
    "    for j in range(n):\n",
    "        pred = np.argmax(p_vals[:, j])\n",
    "        p_set = np.where(p_vals[:, j] > 0.05)[0]\n",
    "        counts += len(p_set)\n",
    "        if lab in missing_label:\n",
    "            if len(p_set) == 0:\n",
    "                cover += 1\n",
    "        else:\n",
    "            if all_label[i] in p_set:\n",
    "                cover += 1\n",
    "    cover_acc[i] = cover / n\n",
    "    avg_count[i] = counts / n\n",
    "cover_accs.append(cover_acc)\n",
    "avg_counts.append(avg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b5406-c586-4155-beb6-766ecb8a4575",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cover_accs)\n",
    "print(avg_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
