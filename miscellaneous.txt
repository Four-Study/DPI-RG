# PCA shows that the 4 dimensions do not have a very good dimension reduction direction
U, S, V = torch.pca_lowrank(fake_Bs)
print(S)
# Correlation coefficient shows they are not correlated to each other.
print(torch.corrcoef(fake_Bs.T))

for i in range(len(all_label)):
    p_vals_class = p_vals_classes[i]
    plt.figure(i, figsize=(16, 4))
    for j in range(len(present_label)):
        plt.subplot(1, len(present_label), j+1)
        plt.xlim(0, 1)
        _ = plt.hist(p_vals_class[j, :], 20)
        prop = np.sum(np.array(p_vals_class[j, :] <= 0.05) / len(p_vals_class[j, :]))
        prop = np.round(prop, 4)
        if all_label[i] in missing_label:
            plt.title('Missing! Label {} in Label {}'.format(all_label[i], present_label[j]))
        else:
            plt.title('Label {} in Label {}\'s net'.format(all_label[i], present_label[j]))
        if all_label[i] == present_label[j]:
            plt.xlabel('Type I Error: {}'.format(prop))
        else:
            plt.xlabel('Power: {}'.format(prop))
    plt.show()

# loss logger
plt.figure(1)
# plt.plot(cur_epochs, train_graphs.loss_G)
plt.plot(train_graphs.cur_epochs, train_graphs.loss_GAN_B2A)
plt.plot(train_graphs.cur_epochs, train_graphs.loss_GAN_A2B)
plt.plot(train_graphs.cur_epochs, train_graphs.loss_G_cycle)
plt.plot(train_graphs.cur_epochs, train_graphs.loss_D_A)
# plt.plot(cur_epochs, train_graphs.loss_D_B)
plt.legend(['GAN A Loss', 'GAN B Loss', 
            'GAN Cycle Loss', 'Discriminator A Loss'])
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.title('cycle GAN Loss')
plt.show()

## save the nets with parameters
path = 'params/' + str(nz) + 'd_' + trainset_A.raw_folder.split('/')[2] + '/'
os.makedirs(path, exist_ok=True)
torch.save(netG_B2A.state_dict(), path + 'netG_B2A.pth')
torch.save(netG_A2B.state_dict(), path + 'netG_A2B.pth')
torch.save(netD_A.state_dict(), path + 'netD_A.pth')

######### Load the Model directly without training
path = 'params/' + str(nz) + 'd_' + trainset_A.raw_folder.split('/')[2] + '/'
netG_B2A.load_state_dict(torch.load(path + 'netG_B2A.pth'))
netG_A2B.load_state_dict(torch.load(path + 'netG_A2B.pth'))
netD_A.load_state_dict(torch.load(path + 'netD_A.pth'))

# show the size and power of our test
null_p_vals = []
alter_p_vals = []
for i in range(n_classes):
    if i not in missing_label:
        null_p_vals += p_vals_class[i]
    else:
        alter_p_vals += p_vals_class[i]
null_p_vals = np.sort(null_p_vals)
alter_p_vals = np.sort(alter_p_vals)

for i in range(n_classes):
    prop = np.sum(np.array(p_vals_class[i]) <= 0.05) / len(p_vals_class[i])
    if i in missing_label:
        print('Label {0}, Power = {1}'.format(i, prop))
    else:
        print('Label {0}, Type I error = {1}'.format(i, prop))

# ###### Create Training Data Set for GeneratorB2C ######
# trainC = TensorDataset(fake_Bs)
# train_loader2 = DataLoader(trainC, batch_size=batch_size, shuffle=True)
# n_epochs2           = 100

###### Create a new Generator to summarize 4 dimensions of B
netG_B2C = GeneratorB2C(ngpu, nz)
netG_B2C.to(device)
netG_B2C.apply(weights_init)
optimizer_G_C = torch.optim.Adam(netG_B2C.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=0.0001)
lr_scheduler_G_C = torch.optim.lr_scheduler.LambdaLR(optimizer_G_C, lr_lambda=LambdaLR(n_epochs, start_epoch, decay_epoch).step)

###### Training B2C ######
for epoch in range(start_epoch, n_epochs2 + 1):
    
    for i, batch in enumerate(train_loader2):
        # Set model input
        fake_B = batch[0]
        bs = len(fake_B)
        if bs < batch_size:
            continue
        real_C = torch.randn(bs, 1, device = device)

        
        ###### Generator B2C ######
        optimizer_G_C.zero_grad()
        
        fake_C = netG_B2C(fake_B)
        # loss
        loss_G_C = criterion_com(fake_C, real_C)
        loss_G_C.backward()

        optimizer_G_C.step()
        ###################################
        
        if i % 200 == 0:
            print('Epoch {}/{}'.format(epoch, n_epochs), 
                  'Iter {}/{}'.format(i + 1, len(train_loader)), 
                  'GAN C Loss:', loss_G_C.item())

    # progress report, burning procedure
    if epoch > 2:
        cur_epochs.append(epoch)
        train_graphs2.loss_G.append(loss_G_C.item())
        
    # Update learning rates
    lr_scheduler_G_C.step()

    
# draw plots
import matplotlib
all_label = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
present_label = [7, 8]

fig, axs = plt.subplots(len(all_label), len(present_label), 
                            figsize=(5*len(present_label), 4*len(all_label)))
matplotlib.rc('xtick', labelsize=15) 
matplotlib.rc('ytick', labelsize=15) 
for i in range(len(all_label)):

    p_vals_class = p_vals_classes[all_label[i]]
    axs[i, 0].set_ylabel(classes[all_label[i]], fontsize = 25)
    for j in range(len(present_label)):

        axs[i, j].set_xlim([0, 1])
        _ = axs[i, j].hist(p_vals_class[present_label[j], :])
        prop = np.sum(np.array(p_vals_class[present_label[j], :] <= 0.05) / len(p_vals_class[present_label[j], :]))
        prop = np.round(prop, 4)
        if all_label[i] == present_label[j]:
            axs[i, j].set_title('Type I Error: {}'.format(prop), fontsize = 20)
        else:
            axs[i, j].set_title('Power: {}'.format(prop), fontsize = 20)

        if i == len(all_label) - 1:
            axs[i, j].set_xlabel(classes[present_label[j]], fontsize = 25)
fig.tight_layout()
# fig.supxlabel('Training Data')
# fig.supylabel('Test Data')
plt.savefig('size_power.pdf', dpi=150)
plt.show()

## display fashion mnist
plt.figure(figsize=(8,8))
plt.axis("off")
sample = A.data[:16, :, :].float().reshape([16, 1, 28, 28])
plt.imshow(np.transpose(vutils.make_grid(sample, nrow = 4, padding = 0, normalize=True),(1, 2, 0)))
plt.savefig('samples.pdf', dpi = 150)