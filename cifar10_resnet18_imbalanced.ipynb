{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c393470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02_20_2212\n"
     ]
    }
   ],
   "source": [
    "## load necessary modules\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.tools import *\n",
    "from utils.losses import *\n",
    "from models.cifar10 import *\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m_%d_%H%M\")\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53617817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # check if gpu is available\n",
    "\n",
    "## load datasets\n",
    "transform    = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),  # Normalize the image\n",
    "                         (0.5, 0.5, 0.5))\n",
    "])\n",
    "train_gen    = dsets.CIFAR10(root=\"./datasets\",train=True, transform=transform, download=True)\n",
    "test_gen     = dsets.CIFAR10(root=\"./datasets\",train=False, transform=transform, download=True)\n",
    "# train_loader = DataLoader(train_gen, batch_size=batch_size, shuffle=True)\n",
    "# test_loader  = DataLoader(test_gen, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1599a5-f845-4653-8753-d0c644d567ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper-parameters\n",
    "n_rep = 10\n",
    "epochs1 = 100\n",
    "epochs2 = 120\n",
    "std = 0.1\n",
    "lr_GI = 5e-4\n",
    "lr_D = 5e-4\n",
    "weight_decay = 0.01\n",
    "batch_size = 250\n",
    "z_dim = 4\n",
    "lambda_mmd = 5.0\n",
    "lambda_gp = 0.1\n",
    "lambda_power = 0.6\n",
    "eta = 3.0\n",
    "dirichlet_alpha = 3.0\n",
    "present_label = list(range(10))\n",
    "missing_label = []\n",
    "all_label     = present_label + missing_label\n",
    "classes       = train_gen.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d527e6e-ddf1-4379-b60e-8e405aa6cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2953 2353 3990 7020 5637 5350 3381 5497 4295 9524]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "# Step 1: Generate Dirichlet distribution weights\n",
    "weights = np.random.dirichlet([dirichlet_alpha] * len(present_label))\n",
    "# Step 2: Calculate the number of samples for each class\n",
    "total_samples = len(train_gen.data)\n",
    "samples_per_class = np.round(weights * total_samples).astype(int)\n",
    "# Adjust to make sure the total is equal to the original test set size\n",
    "samples_per_class[-1] = total_samples - np.sum(samples_per_class[:-1])\n",
    "sampled_idxs = []\n",
    "for idx, lab in enumerate(present_label):    \n",
    "    if torch.is_tensor(train_gen.targets):\n",
    "        class_idxs = torch.where(train_gen.targets == lab)[0] \n",
    "    else:\n",
    "        class_idxs = torch.where(torch.Tensor(train_gen.targets) == lab)[0] \n",
    "    temp = np.random.choice(class_idxs, size=samples_per_class[idx], replace=True)\n",
    "    sampled_idxs.append(torch.Tensor(temp).int())\n",
    "print(samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de72efba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Class 0 Started\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch = 0\n",
      "GI loss:  0.697925\n",
      "MMD loss: 0.047669\n",
      "D loss:   -0.000997\n",
      "gp loss:  0.081525\n",
      "power loss: 0.754621\n",
      "Epoch = 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m## train for the first time\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m train_al(netI, netG, netD, optim_I, optim_G, optim_D,\n\u001b[1;32m     37\u001b[0m          train_gen, train_loader, batch_size, \u001b[38;5;241m0\u001b[39m, epochs1, \n\u001b[1;32m     38\u001b[0m          z_dim, device, lab, present_label, all_label, \n\u001b[1;32m     39\u001b[0m          lambda_gp, lambda_power, lambda_mmd \u001b[38;5;241m=\u001b[39m lambda_mmd,\n\u001b[1;32m     40\u001b[0m          img_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, nc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, eta \u001b[38;5;241m=\u001b[39m eta, sampled_idxs \u001b[38;5;241m=\u001b[39m sampled_idxs, \n\u001b[1;32m     41\u001b[0m          lr_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m, trace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, critic_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, critic_iter_d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, critic_iter_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m## find out fake_zs\u001b[39;00m\n\u001b[1;32m     44\u001b[0m fake_zs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Git/FPI/utils/tools.py:68\u001b[0m, in \u001b[0;36mtrain_al\u001b[0;34m(netI, netG, netD, optim_I, optim_G, optim_D, train_gen, train_loader, batch_size, start_epoch, end_epoch, z_dim, device, lab, present_label, all_label, lambda_gp, lambda_power, lambda_mmd, eta, sample_sizes, sampled_idxs, img_size, nc, critic_iter, critic_iter_d, critic_iter_p, lr_decay, trace)\u001b[0m\n\u001b[1;32m     66\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;28mlen\u001b[39m(images), z_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     67\u001b[0m fake_z \u001b[38;5;241m=\u001b[39m netI(x)\n\u001b[0;32m---> 68\u001b[0m fake_x \u001b[38;5;241m=\u001b[39m netG(z)\n\u001b[1;32m     69\u001b[0m netI\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     70\u001b[0m netG\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(replicas)])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:102\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    100\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m--> 102\u001b[0m         thread\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:1112\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:1132\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1133\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ************************\n",
    "# *** DPI-RG Algorithm ***\n",
    "# ************************\n",
    "\n",
    "cover_accs = []\n",
    "avg_counts = []\n",
    "\n",
    "# for rep in range(n_rep):\n",
    "T_trains = []\n",
    "for lab in present_label:\n",
    "        \n",
    "    print('-'*100)\n",
    "    print(f'Class {lab} Started')\n",
    "    print('-'*100)\n",
    "    ## initialize models\n",
    "    netI = I_CIFAR10(nz=z_dim)\n",
    "    netG = G_CIFAR10(nz=z_dim, ngf=16)\n",
    "    netD = D_CIFAR10(nz=z_dim, ndf=16)\n",
    "    netI = netI.to(device)\n",
    "    netG = netG.to(device)\n",
    "    netD = netD.to(device)\n",
    "    netI = nn.DataParallel(netI)\n",
    "    netG = nn.DataParallel(netG)\n",
    "    netD = nn.DataParallel(netD)\n",
    "\n",
    "    ## set up optimizers\n",
    "    optim_I = optim.Adam(netI.parameters(), lr=lr_GI, betas=(0.5, 0.999))\n",
    "    optim_G = optim.Adam(netG.parameters(), lr=lr_GI, betas=(0.5, 0.999))\n",
    "    optim_D = optim.Adam(netD.parameters(), lr=lr_D, betas=(0.5, 0.999), \n",
    "                         weight_decay=weight_decay)\n",
    "    ## filter data for each label and train them respectively\n",
    "    train_data = torch.utils.data.Subset(train_gen, sampled_idxs[lab])\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    ## train for the first time\n",
    "    train_al(netI, netG, netD, optim_I, optim_G, optim_D,\n",
    "             train_gen, train_loader, batch_size, 0, epochs1, \n",
    "             z_dim, device, lab, present_label, all_label, \n",
    "             lambda_gp, lambda_power, lambda_mmd = lambda_mmd,\n",
    "             img_size = 32, nc = 3, eta = eta, sampled_idxs = sampled_idxs, \n",
    "             lr_decay = 40, trace=True, critic_iter = 10, critic_iter_d = 10, critic_iter_p = 5)\n",
    "\n",
    "    ## find out fake_zs\n",
    "    fake_zs = []\n",
    "    with torch.no_grad(): \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            x, _ = batch\n",
    "            fake_z = netI(x.to(device))\n",
    "            fake_zs.append(fake_z)\n",
    "    fake_zs = torch.cat(fake_zs)\n",
    "    ## get the empirical distribution for each label\n",
    "    T_train = torch.sqrt(torch.sum(fake_zs ** 2, dim=1) + 1)\n",
    "\n",
    "    ## get powers to determine new sample sizes\n",
    "    powers = []\n",
    "    for cur_lab in present_label:    \n",
    "        if cur_lab != lab:\n",
    "            # fake_Cs for this class\n",
    "            # if torch.is_tensor(train_gen.targets):\n",
    "            #     idxs3 = torch.where(train_gen.targets == cur_lab)[0] \n",
    "            # else:\n",
    "            #     idxs3 = torch.where(torch.Tensor(train_gen.targets) == cur_lab)[0] \n",
    "            idxs3 = sampled_idxs[cur_lab]\n",
    "            train_data3 = torch.utils.data.Subset(train_gen, idxs3)\n",
    "            train_loader3  = DataLoader(train_data3, batch_size=batch_size, shuffle=False)\n",
    "            p_vals = torch.zeros(len(idxs3)) \n",
    "            fake_zs = torch.zeros(len(idxs3))\n",
    "            em_len = len(T_train)\n",
    "\n",
    "            for i, batch in enumerate(train_loader3):\n",
    "                x, _ = batch\n",
    "                fake_z = netI(x.to(device))\n",
    "                T_batch = torch.sqrt(torch.sum(fake_z ** 2, dim=1) + 1)\n",
    "\n",
    "                # compute p-value for each sample\n",
    "                for j in range(len(fake_z)):\n",
    "                    p1 = torch.sum(T_train > T_batch[j]) / em_len\n",
    "                    p = p1\n",
    "                    # calculate the p-value and put it in the corresponding list\n",
    "                    p_vals[i * batch_size + j] = p.item()\n",
    "            powers.append(np.sum(np.array(p_vals) <= 0.05) / len(idxs3))\n",
    "            \n",
    "    sample_sizes = max(powers) - powers + 0.05\n",
    "    sample_sizes = (sample_sizes / sum(sample_sizes) * len(idxs3)).astype(int)\n",
    "    # print(sample_sizes)\n",
    "    ## train for the second time according to the calculated sample sizes\n",
    "    train_al(netI, netG, netD, optim_I, optim_G, optim_D,\n",
    "             train_gen, train_loader, batch_size, epochs1, epochs2, \n",
    "             z_dim, device, lab, present_label, all_label, \n",
    "             lambda_gp, lambda_power, lambda_mmd = lambda_mmd, sample_sizes = sample_sizes, \n",
    "             img_size = 32, nc = 3, eta = eta, sampled_idxs = sampled_idxs, \n",
    "             lr_decay = None, trace = True, critic_iter = 5, critic_iter_d = 5, critic_iter_p = 5)\n",
    "    \n",
    "    ## find out fake_zs\n",
    "    fake_zs = []\n",
    "    with torch.no_grad(): \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            x, _ = batch\n",
    "            fake_z = netI(x.to(device))\n",
    "            fake_zs.append(fake_z)\n",
    "    fake_zs = torch.cat(fake_zs)\n",
    "    ## get the empirical distribution for each label\n",
    "    T_train = torch.sqrt(torch.sum(fake_zs ** 2, dim=1) + 1)\n",
    "    T_trains.append(T_train)\n",
    "\n",
    "    ## save net and graphs for each label\n",
    "    model_save_file = f'cifar10_param/{timestamp}_class{lab}.pt'\n",
    "    torch.save(netI.state_dict(), model_save_file)\n",
    "    del netI\n",
    "    print('-'*100)\n",
    "    print(f'Class {lab} Finished')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef165e1-7090-454d-ac94-d74eec332227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training for verification\n",
    "# all_p_vals  = []\n",
    "# all_fake_Ts = []\n",
    "\n",
    "# for lab in all_label:    \n",
    "#     if torch.is_tensor(train_gen.targets):\n",
    "#         idxs2 = torch.where(train_gen.targets == lab)[0] \n",
    "#     else:\n",
    "#         idxs2 = torch.where(torch.Tensor(train_gen.targets) == lab)[0] \n",
    "#     test_data = torch.utils.data.Subset(train_gen, idxs2)\n",
    "#     test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # p_vals and fake_zs store p-values, fake_zs for the current iteration\n",
    "#     fake_Ts = torch.zeros(len(present_label), len(idxs2))\n",
    "#     p_vals = torch.zeros(len(present_label), len(idxs2))\n",
    "\n",
    "#     for pidx in range(len(present_label)):\n",
    "#         T_train = T_trains[pidx]\n",
    "#         em_len = len(T_train)\n",
    "#         netI = I_CIFAR10(nz=z_dim)\n",
    "#         netI = netI.to(device)\n",
    "#         netI = torch.nn.DataParallel(netI)\n",
    "#         model_save_file = f'cifar10_param/{timestamp}_class{present_label[pidx]}.pt'\n",
    "#         netI.load_state_dict(torch.load(model_save_file))\n",
    "        \n",
    "#         for i, batch in enumerate(test_loader):\n",
    "#             images, y = batch\n",
    "#             x = images.view(-1, 3, 32 * 32).to(device)\n",
    "#             fake_z = netI(x)\n",
    "#             T_batch = torch.sqrt(torch.sum(torch.square(fake_z), 1) + 1)\n",
    "#             ## compute p-value for each sample\n",
    "#             for j in range(len(fake_z)):\n",
    "#                 p1 = torch.sum(T_train > T_batch[j]) / em_len\n",
    "#                 # p2 = torch.sum(T_train < T_batch[j]) / em_len\n",
    "#                 # p = 2*min(p1,p2)\n",
    "#                 p = p1\n",
    "#                 # calculate the p-value and put it in the corresponding list\n",
    "#                 fake_Ts[pidx, i * batch_size + j] = T_batch[j].item()\n",
    "#                 p_vals[pidx, i * batch_size + j] = p.item()\n",
    "\n",
    "#     all_p_vals.append(np.array(p_vals))\n",
    "#     ## concatenate torch data\n",
    "#     all_fake_Ts.append(np.array(fake_Ts))\n",
    "#     # print('Finished Label {}'.format(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f6b7f-a433-41e2-9de5-f313748dd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_T(all_fake_Ts, present_label, all_label, missing_label, z_dim, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5650706-a3e7-47f2-ace7-4d4dd0c340dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_p(all_p_vals, present_label, all_label, missing_label, z_dim, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bb3e3-7301-4263-a533-21e56b982358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test data set\n",
    "all_p_vals  = []\n",
    "all_fake_Ts = []\n",
    "\n",
    "for lab in all_label:\n",
    "    if torch.is_tensor(test_gen.targets):\n",
    "        idxs2 = torch.where(test_gen.targets == lab)[0]\n",
    "    else:\n",
    "        idxs2 = torch.where(torch.Tensor(test_gen.targets) == lab)[0]\n",
    "    test_data = torch.utils.data.Subset(test_gen, idxs2)\n",
    "    test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # p_vals and fake_zs store p-values, fake_zs for the current iteration\n",
    "    fake_Ts = torch.zeros(len(present_label), len(idxs2))\n",
    "    p_vals = torch.zeros(len(present_label), len(idxs2)) \n",
    "\n",
    "    for pidx in range(len(present_label)):\n",
    "        T_train = T_trains[pidx]\n",
    "        em_len = len(T_train)\n",
    "        netI = I_CIFAR10(nz=z_dim)\n",
    "        netI = netI.to(device)\n",
    "        netI = torch.nn.DataParallel(netI)\n",
    "        model_save_file = f'cifar10_param/{timestamp}_class{present_label[pidx]}.pt'\n",
    "        netI.load_state_dict(torch.load(model_save_file))\n",
    "        \n",
    "        for i, batch in enumerate(test_loader):\n",
    "            images, y = batch\n",
    "            x = images.view(-1, 3, 32 * 32).to(device)\n",
    "            fake_z = netI(x)\n",
    "            T_batch = torch.sqrt(torch.sum(torch.square(fake_z), 1) + 1) \n",
    "            ## compute p-value for each sample\n",
    "            for j in range(len(fake_z)):\n",
    "                p1 = torch.sum(T_train > T_batch[j]) / em_len\n",
    "                p = p1\n",
    "                # calculate the p-value and put it in the corresponding list\n",
    "                fake_Ts[pidx, i * batch_size + j] = T_batch[j].item()\n",
    "                p_vals[pidx, i * batch_size + j] = p.item()\n",
    "\n",
    "    all_p_vals.append(np.array(p_vals))\n",
    "    ## concatenate torch data\n",
    "    all_fake_Ts.append(np.array(fake_Ts))\n",
    "    # print('Finished Label {}'.format(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a96a0-c2e2-4ccc-8c8a-8d9314de1c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_T(all_fake_Ts, present_label, all_label, missing_label, z_dim, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b7d2a-f2b7-41c5-adbe-140aa48529af",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_p(all_p_vals, present_label, all_label, missing_label, z_dim, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c52517-29f4-472f-9064-2ae64561f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_accs = []\n",
    "avg_errors = []\n",
    "n_rep = 20\n",
    "for rep in range(n_rep):\n",
    "    # Step 1: Generate Dirichlet distribution weights\n",
    "    weights = np.random.dirichlet([dirichlet_alpha] * len(all_label))\n",
    "    # Step 2: Calculate the number of samples for each class\n",
    "    total_samples = len(test_gen.data)\n",
    "    samples_per_class = np.round(weights * total_samples).astype(int)\n",
    "    # Adjust to make sure the total is equal to the original test set size\n",
    "    samples_per_class[-1] = total_samples - np.sum(samples_per_class[:-1])\n",
    "\n",
    "    ## test data set\n",
    "    sampled_p_vals  = []\n",
    "    \n",
    "    for idx, lab in enumerate(all_label):    \n",
    "        \n",
    "        p_vals = all_p_vals[idx]\n",
    "        n = p_vals.shape[1]\n",
    "        class_idxs = np.random.choice(np.arange(n), size=samples_per_class[idx], replace=True)\n",
    "        # class_idxs = idxs2\n",
    "        sampled_p_vals.append(p_vals[:,class_idxs])\n",
    "    \n",
    "    # cover_acc = torch.zeros(len(all_label))\n",
    "    # avg_error = torch.zeros(len(all_label))\n",
    "    cover = 0.0\n",
    "    error = 0.0\n",
    "    for i, lab in enumerate(all_label):\n",
    "        p_vals = sampled_p_vals[i]\n",
    "        n = p_vals.shape[1]\n",
    "        for j in range(n):\n",
    "            pred = np.argmax(p_vals[:, j])\n",
    "            p_set = np.where(p_vals[:, j] > 0.05)[0]\n",
    "            # counts += len(p_set)\n",
    "            if lab in missing_label:\n",
    "                error += len(p_set)\n",
    "                if len(p_set) == 0:\n",
    "                    cover += 1\n",
    "            else:\n",
    "                error += abs(len(p_set) - 1) \n",
    "                if all_label[i] in p_set:\n",
    "                    cover += 1\n",
    "        # cover_acc[i] = cover / n \n",
    "        # avg_error[i] = error / n \n",
    "    cover_acc = cover / total_samples \n",
    "    avg_error = error / total_samples \n",
    "    cover_accs.append(cover_acc)\n",
    "    avg_errors.append(avg_error)\n",
    "    # print(f'rep = {rep+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b5406-c586-4155-beb6-766ecb8a4575",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cover_accs))\n",
    "print(np.std(cover_accs))\n",
    "print(np.mean(avg_errors))\n",
    "print(np.std(avg_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbced23-973f-4ceb-a1ae-0e5e9999c196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
