{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c393470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024_02_13_1652\n"
     ]
    }
   ],
   "source": [
    "## load necessary modules\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.tools import *\n",
    "from utils.losses import *\n",
    "from models.mnist import *\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m_%d_%H%M\")\n",
    "timestamp = '2024_02_13_1652'\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53617817",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "# torch.manual_seed(random_seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # check if gpu is available\n",
    "\n",
    "## load datasets\n",
    "transform    = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_gen    = dsets.FashionMNIST(root=\"./datasets\",train=True, transform=transform, download=True)\n",
    "test_gen     = dsets.FashionMNIST(root=\"./datasets\",train=False, transform=transform, download=True)\n",
    "\n",
    "\n",
    "## initial empty lists for training progress\n",
    "# primal_loss_GI = []\n",
    "# dual_loss_GI = []\n",
    "# primal_loss = []\n",
    "# dual_loss = []\n",
    "# primal_loss_z = []\n",
    "# loss_mmd = []\n",
    "# gp = []\n",
    "# re = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1599a5-f845-4653-8753-d0c644d567ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper-parameters\n",
    "n_rep = 20\n",
    "epochs1 = 50\n",
    "epochs2 = 50\n",
    "std = 0.1\n",
    "lr_GI = 5e-4\n",
    "lr_D = 5e-4 * 5\n",
    "weight_decay = 0.01\n",
    "batch_size = 250\n",
    "z_dim = 5\n",
    "lambda_mmd = 2.0\n",
    "lambda_gp = 0.1\n",
    "lambda_power = 1.0\n",
    "eta = 2.5\n",
    "dirichlet_alpha = 3.0\n",
    "present_label = list(range(10))\n",
    "missing_label = []\n",
    "all_label     = present_label + missing_label\n",
    "classes       = train_gen.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2be210-4761-4378-b702-c226ed2839c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3647 5484 8784 6420 8182 7264 1649 8615 6827 3128]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12345)\n",
    "# Step 1: Generate Dirichlet distribution weights\n",
    "weights = np.random.dirichlet([dirichlet_alpha] * len(present_label))\n",
    "# Step 2: Calculate the number of samples for each class\n",
    "total_samples = len(train_gen.data)\n",
    "samples_per_class = np.round(weights * total_samples).astype(int)\n",
    "# Adjust to make sure the total is equal to the original test set size\n",
    "samples_per_class[-1] = total_samples - np.sum(samples_per_class[:-1])\n",
    "sampled_idxs = []\n",
    "for idx, lab in enumerate(present_label):    \n",
    "    if torch.is_tensor(train_gen.targets):\n",
    "        class_idxs = torch.where(train_gen.targets == lab)[0] \n",
    "    else:\n",
    "        class_idxs = torch.where(torch.Tensor(train_gen.targets) == lab)[0] \n",
    "    temp = np.random.choice(class_idxs, size=samples_per_class[idx], replace=True)\n",
    "    sampled_idxs.append(torch.Tensor(temp).int())\n",
    "print(samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de72efba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0\n",
      "Class 1\n",
      "Class 2\n",
      "Class 3\n",
      "Class 4\n",
      "Class 5\n",
      "Class 6\n",
      "Class 7\n",
      "Class 8\n",
      "Class 9\n"
     ]
    }
   ],
   "source": [
    "# ************************\n",
    "# *** DPI-RG Algorithm ***\n",
    "# ************************\n",
    "\n",
    "cover_accs = []\n",
    "avg_counts = []\n",
    "\n",
    "# for rep in range(n_rep):\n",
    "T_trains = []\n",
    "for lab in present_label:\n",
    "    ## initialize models\n",
    "    netI = I_MNIST2(nz=z_dim)\n",
    "    netG = G_MNIST(nz=z_dim)\n",
    "    netD = D_MNIST(nz=z_dim)\n",
    "    netI = netI.to(device)\n",
    "    netG = netG.to(device)\n",
    "    netD = netD.to(device)\n",
    "    netI = nn.DataParallel(netI)\n",
    "    netG = nn.DataParallel(netG)\n",
    "    netD = nn.DataParallel(netD)\n",
    "    model_save_file = f'fmnist_param/{timestamp}_class{lab}.pt'\n",
    "    netI.load_state_dict(torch.load(model_save_file))\n",
    "\n",
    "    # ## set up optimizers\n",
    "    # optim_I = optim.Adam(netI.parameters(), lr=lr_GI, betas=(0.5, 0.999))\n",
    "    # optim_G = optim.Adam(netG.parameters(), lr=lr_GI, betas=(0.5, 0.999))\n",
    "    # optim_D = optim.Adam(netD.parameters(), lr=lr_D,  betas=(0.5, 0.999), \n",
    "    #                      weight_decay=weight_decay)\n",
    "    ## filter data for each label and train them respectively\n",
    "    train_data = torch.utils.data.Subset(train_gen, sampled_idxs[lab])\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # ## train for the first time\n",
    "    # train_al(netI, netG, netD, optim_I, optim_G, optim_D,\n",
    "    #          train_gen, train_loader, batch_size, 0, epochs1, \n",
    "    #          z_dim, device, lab, present_label, all_label, \n",
    "    #          lambda_gp, lambda_power, lambda_mmd = lambda_mmd, eta = eta, \n",
    "    #          sampled_idxs = sampled_idxs, trace=True)\n",
    "    # ## find out fake_zs\n",
    "    # fake_zs = []\n",
    "    # with torch.no_grad(): \n",
    "    #     for i, batch in enumerate(train_loader):\n",
    "    #         x, _ = batch\n",
    "    #         fake_z = netI(x.to(device))\n",
    "    #         fake_zs.append(fake_z)\n",
    "    # fake_zs = torch.cat(fake_zs)\n",
    "    # ## get the empirical distribution for each label\n",
    "    # T_train = torch.sqrt(torch.sum(fake_zs ** 2, dim=1) + 1)\n",
    "\n",
    "    # ## get powers to determine new sample sizes\n",
    "    # powers = []\n",
    "    # for cur_lab in present_label:    \n",
    "    #     if cur_lab != lab:\n",
    "    #         # fake_Cs for this class\n",
    "    #         # if torch.is_tensor(train_gen.targets):\n",
    "    #         #     idxs3 = torch.where(train_gen.targets == cur_lab)[0] \n",
    "    #         # else:\n",
    "    #         #     idxs3 = torch.where(torch.Tensor(train_gen.targets) == cur_lab)[0] \n",
    "    #         idxs3 = sampled_idxs[cur_lab]\n",
    "    #         train_data3 = torch.utils.data.Subset(train_gen, idxs3)\n",
    "    #         train_loader3  = DataLoader(train_data3, batch_size=batch_size, shuffle=False)\n",
    "    #         p_vals = torch.zeros(len(idxs3)) \n",
    "    #         fake_zs = torch.zeros(len(idxs3))\n",
    "    #         em_len = len(T_train)\n",
    "\n",
    "    #         for i, batch in enumerate(train_loader3):\n",
    "    #             x, _ = batch\n",
    "    #             fake_z = netI(x.to(device))\n",
    "    #             T_batch = torch.sqrt(torch.sum(fake_z ** 2, dim=1) + 1)\n",
    "\n",
    "    #             # compute p-value for each sample\n",
    "    #             for j in range(len(fake_z)):\n",
    "    #                 p1 = torch.sum(T_train > T_batch[j]) / em_len\n",
    "    #                 p = p1\n",
    "    #                 # calculate the p-value and put it in the corresponding list\n",
    "    #                 p_vals[i * batch_size + j] = p.item()\n",
    "    #         powers.append(np.sum(np.array(p_vals) <= 0.05) / len(idxs3))\n",
    "            \n",
    "    # sample_sizes = max(powers) - powers + 0.05\n",
    "    # sample_sizes = (sample_sizes / sum(sample_sizes) * len(sampled_idxs[lab])).astype(int)\n",
    "    # ## train for the second time according to the calculated sample sizes\n",
    "    # train_al(netI, netG, netD, optim_I, optim_G, optim_D,\n",
    "    #          train_gen, train_loader, batch_size, epochs1, epochs2, \n",
    "    #          z_dim, device, lab, present_label, all_label, \n",
    "    #          lambda_gp, lambda_power, lambda_mmd = lambda_mmd, eta = eta, \n",
    "    #          sample_sizes = sample_sizes, sampled_idxs = sampled_idxs, trace=True)\n",
    "    \n",
    "    ## find out fake_zs\n",
    "    fake_zs = []\n",
    "    with torch.no_grad(): \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            x, _ = batch\n",
    "            fake_z = netI(x.to(device))\n",
    "            fake_zs.append(fake_z)\n",
    "    fake_zs = torch.cat(fake_zs)\n",
    "    ## get the empirical distribution for each label\n",
    "    T_train = torch.sqrt(torch.sum(fake_zs ** 2, dim=1) + 1)\n",
    "    T_trains.append(T_train)\n",
    "\n",
    "    ## save net and graphs for each label\n",
    "    # model_save_file = f'fmnist_param/{timestamp}_class{lab}.pt'\n",
    "    # torch.save(netI.state_dict(), model_save_file)\n",
    "    del netI\n",
    "    print(f'Class {lab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148bb3e3-7301-4263-a533-21e56b982358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## test data set\n",
    "# all_p_vals  = []\n",
    "# all_fake_Ts = []\n",
    "\n",
    "# for lab in all_label:    \n",
    "#     if torch.is_tensor(test_gen.targets):\n",
    "#         idxs2 = torch.where(test_gen.targets == lab)[0] \n",
    "#     else:\n",
    "#         idxs2 = torch.where(torch.Tensor(test_gen.targets) == lab)[0] \n",
    "#     test_data = torch.utils.data.Subset(test_gen, idxs2)\n",
    "#     test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # p_vals and fake_zs store p-values, fake_zs for the current iteration\n",
    "#     fake_Ts = torch.zeros(len(present_label), len(idxs2))\n",
    "#     p_vals = torch.zeros(len(present_label), len(idxs2)) \n",
    "\n",
    "#     for pidx in range(len(present_label)):\n",
    "#         T_train = T_trains[pidx]\n",
    "#         em_len = len(T_train)\n",
    "#         netI = I_MNIST(nz=z_dim)\n",
    "#         netI = netI.to(device)\n",
    "#         netI = torch.nn.DataParallel(netI)\n",
    "#         model_save_file = f'fmnist_param/{timestamp}_class{present_label[pidx]}.pt'\n",
    "#         netI.load_state_dict(torch.load(model_save_file))\n",
    "        \n",
    "#         for i, batch in enumerate(test_loader):\n",
    "#             images, y = batch\n",
    "#             x = images.view(-1, 1, 28 * 28).to(device)\n",
    "#             fake_z = netI(x)\n",
    "#             T_batch = torch.sqrt(torch.sum(torch.square(fake_z), 1) + 1) \n",
    "#             ## compute p-value for each sample\n",
    "#             for j in range(len(fake_z)):\n",
    "#                 p1 = torch.sum(T_train > T_batch[j]) / em_len\n",
    "#                 p = p1\n",
    "#                 # calculate the p-value and put it in the corresponding list\n",
    "#                 fake_Ts[pidx, i * batch_size + j] = T_batch[j].item()\n",
    "#                 p_vals[pidx, i * batch_size + j] = p.item()\n",
    "\n",
    "#     all_p_vals.append(np.array(p_vals))\n",
    "#     ## concatenate torch data\n",
    "#     all_fake_Ts.append(np.array(fake_Ts))\n",
    "#     # print('Finished Label {}'.format(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac230279-10d7-48dc-b453-3edbea7283a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_fake_T(all_fake_Ts, present_label, all_label, missing_label, z_dim, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf3b7d2a-f2b7-41c5-adbe-140aa48529af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_p(all_p_vals, present_label, all_label, missing_label, z_dim, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c52517-29f4-472f-9064-2ae64561f313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rep = 1\n",
      "rep = 2\n",
      "rep = 3\n",
      "rep = 4\n",
      "rep = 5\n",
      "rep = 6\n",
      "rep = 7\n",
      "rep = 8\n",
      "rep = 9\n",
      "rep = 10\n",
      "rep = 11\n",
      "rep = 12\n",
      "rep = 13\n",
      "rep = 14\n",
      "rep = 15\n",
      "rep = 16\n",
      "rep = 17\n",
      "rep = 18\n",
      "rep = 19\n",
      "rep = 20\n"
     ]
    }
   ],
   "source": [
    "cover_accs = []\n",
    "avg_errors = []\n",
    "\n",
    "for rep in range(n_rep):\n",
    "    # Step 1: Generate Dirichlet distribution weights\n",
    "    weights = np.random.dirichlet([dirichlet_alpha] * len(all_label))\n",
    "    # Step 2: Calculate the number of samples for each class\n",
    "    total_samples = len(test_gen.data)\n",
    "    samples_per_class = np.round(weights * total_samples).astype(int)\n",
    "    # Adjust to make sure the total is equal to the original test set size\n",
    "    samples_per_class[-1] = total_samples - np.sum(samples_per_class[:-1])\n",
    "\n",
    "    ## test data set\n",
    "    all_p_vals  = []\n",
    "    all_fake_Ts = []\n",
    "    \n",
    "    for lab in all_label:    \n",
    "        if torch.is_tensor(test_gen.targets):\n",
    "            idxs2 = torch.where(test_gen.targets == lab)[0] \n",
    "        else:\n",
    "            idxs2 = torch.where(torch.Tensor(test_gen.targets) == lab)[0] \n",
    "        test_data = torch.utils.data.Subset(test_gen, idxs2)\n",
    "        test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "        # p_vals and fake_zs store p-values, fake_zs for the current iteration\n",
    "        fake_Ts = torch.zeros(len(present_label), len(idxs2))\n",
    "        p_vals = torch.zeros(len(present_label), len(idxs2)) \n",
    "    \n",
    "        for pidx in range(len(present_label)):\n",
    "            T_train = T_trains[pidx]\n",
    "            em_len = len(T_train)\n",
    "            netI = I_MNIST2(nz=z_dim)\n",
    "            netI = netI.to(device)\n",
    "            netI = torch.nn.DataParallel(netI)\n",
    "            model_save_file = f'fmnist_param/{timestamp}_class{present_label[pidx]}.pt'\n",
    "            netI.load_state_dict(torch.load(model_save_file))\n",
    "            \n",
    "            for i, batch in enumerate(test_loader):\n",
    "                images, y = batch\n",
    "                x = images.view(-1, 1, 28 * 28).to(device)\n",
    "                fake_z = netI(x)\n",
    "                T_batch = torch.sqrt(torch.sum(torch.square(fake_z), 1) + 1) \n",
    "                ## compute p-value for each sample\n",
    "                for j in range(len(fake_z)):\n",
    "                    p1 = torch.sum(T_train > T_batch[j]) / em_len\n",
    "                    p = p1\n",
    "                    # calculate the p-value and put it in the corresponding list\n",
    "                    fake_Ts[pidx, i * batch_size + j] = T_batch[j].item()\n",
    "                    p_vals[pidx, i * batch_size + j] = p.item()\n",
    "    \n",
    "        all_p_vals.append(np.array(p_vals))\n",
    "        ## concatenate torch data\n",
    "        all_fake_Ts.append(np.array(fake_Ts))\n",
    "        # print('Finished Label {}'.format(lab))\n",
    "    \n",
    "    # cover_acc = torch.zeros(len(all_label))\n",
    "    # avg_error = torch.zeros(len(all_label))\n",
    "    cover = 0.0\n",
    "    error = 0.0\n",
    "    for i, lab in enumerate(all_label):\n",
    "        p_vals = all_p_vals[i]\n",
    "        n = p_vals.shape[1]\n",
    "        for j in range(n):\n",
    "            pred = np.argmax(p_vals[:, j])\n",
    "            p_set = np.where(p_vals[:, j] > 0.05)[0]\n",
    "            # counts += len(p_set)\n",
    "            if lab in missing_label:\n",
    "                error += len(p_set)\n",
    "                if len(p_set) == 0:\n",
    "                    cover += 1\n",
    "            else:\n",
    "                error += abs(len(p_set) - 1) \n",
    "                if all_label[i] in p_set:\n",
    "                    cover += 1\n",
    "        # cover_acc[i] = cover / n \n",
    "        # avg_error[i] = error / n \n",
    "    cover_acc = cover / total_samples \n",
    "    avg_error = error / total_samples \n",
    "    cover_accs.append(cover_acc)\n",
    "    avg_errors.append(avg_error)\n",
    "    print(f'rep = {rep+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e53b5406-c586-4155-beb6-766ecb8a4575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9452999999999999\n",
      "0.05469999999999999\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cover_accs))\n",
    "print(np.mean(avg_errors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
